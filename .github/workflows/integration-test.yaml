---
name: Integration test

on:
  pull_request

jobs:
  minikube-ci:
    name: Integration test (with${{ matrix.audience == false && 'out' || '' }} audience parameter)
    runs-on: ubuntu-latest
    strategy:
      matrix:
        audience: [false, true]
    steps:
      - name: Check out the repo
        uses: actions/checkout@v6

      - name: Start minikube
        uses: medyagh/setup-minikube@latest

      - name: Install Dependencies
        run: |
          # Install yq
          wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/bin/yq && chmod +x /usr/bin/yq
          yq --version

      - name: Build csi-snapshot-metadata container image
        run: |
          make build-csi-snapshot-metadata
          minikube image build -f ./cmd/csi-snapshot-metadata/Dockerfile -t gcr.io/k8s-staging-sig-storage/csi-snapshot-metadata:test .

      - name: Deploy snapshot-controller
        run: |
          ./scripts/deploy-snapshot-controller.sh deploy

      - name: Configure audience parameter
        run: |
          if [ "${{ matrix.audience }}" = "true" ]; then
            echo "Enabling audience parameter"
            sed -i 's/# - "--audience=test-backup-client"/- "--audience=test-backup-client"/' deploy/example/csi-driver/csi-driver-with-snapshot-metadata-sidecar.yaml
          fi

      - name: Deploy csi-hostpath-driver
        run: |
          kubectl apply -f ./client/config/crd/cbt.storage.k8s.io_snapshotmetadataservices.yaml

          git clone https://github.com/kubernetes-csi/csi-driver-host-path.git ~/csi-driver-host-path

          CSI_SNAPSHOT_METADATA_REGISTRY="gcr.io/k8s-staging-sig-storage" UPDATE_RBAC_RULES="false" CSI_SNAPSHOT_METADATA_TAG="test" SNAPSHOT_METADATA_TESTS=true HOSTPATHPLUGIN_REGISTRY="gcr.io/k8s-staging-sig-storage" HOSTPATHPLUGIN_TAG="canary" ~/csi-driver-host-path/deploy/kubernetes-latest/deploy.sh

          kubectl apply -f ./deploy/example/csi-driver/testdata/

          echo "Restart CSI driver pods to reload the new TLS certificates from testdata"
          kubectl delete po -l app.kubernetes.io/name=csi-hostpathplugin

          kubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=csi-hostpathplugin --timeout=300s
          kubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=csi-hostpath-socat --timeout=300s

      - name: Deploy backup client app resources
        run: |
          kubectl create -f deploy/snapshot-metadata-client-cluster-role.yaml
          kubectl create serviceaccount backup-app-service-account
          kubectl create clusterrolebinding csi-client-cluster-role-binding --clusterrole=external-snapshot-metadata-client-runner --serviceaccount=default:backup-app-service-account

          go build -o snapshot-metadata-lister ./tools/snapshot-metadata-lister/main.go
          go build -o snapshot-metadata-verifier ./tools/snapshot-metadata-verifier/main.go

          ./snapshot-metadata-lister -h

      - name: Execute tests
        run: |
          echo "Create a raw block PVC"
          kubectl create -f ~/csi-driver-host-path/examples/csi-storageclass.yaml
          yq -i '.spec.resources.requests.storage = "1Mi"' ~/csi-driver-host-path/examples/csi-pvc-block.yaml
          kubectl create -f ~/csi-driver-host-path/examples/csi-pvc-block.yaml

          echo "Create a pod consuming raw block PVC"
          # Failed to pull image "gcr.io/google_containers/busybox":
          # [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2,
          # schema 1 support is disabled by default and will be removed in an upcoming release.
          # Suggest the author of gcr.io/google_containers/busybox:latest to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2.
          # More information at https://docs.docker.com/go/deprecated-image-specs/
          yq -i '.spec.containers[0].image = "busybox:latest"' ~/csi-driver-host-path/examples/csi-pod-block.yaml
          yq -i '.spec.containers[0].volumeDevices[0].devicePath = "/dev/block"' ~/csi-driver-host-path/examples/csi-pod-block.yaml
          kubectl create -f ~/csi-driver-host-path/examples/csi-pod-block.yaml
          kubectl wait --for=condition=Ready pod/pod-raw --timeout=300s

          echo "Write data into pod"
          kubectl exec -i pod-raw -- sh -c "dd if=/dev/urandom of=/dev/block bs=4K count=1 oflag=direct"

          echo "Take snaphot snap-1"
          yq -i '.metadata.name = "snap-1"' ~/csi-driver-host-path/examples/csi-snapshot-v1.yaml
          yq -i '.spec.source.persistentVolumeClaimName = "pvc-raw"' ~/csi-driver-host-path/examples/csi-snapshot-v1.yaml
          kubectl create -f ~/csi-driver-host-path/examples/csi-snapshot-v1.yaml
          kubectl wait volumesnapshot  snap-1 --for=jsonpath='{.status.readyToUse}'=true --timeout=300s
          
          echo "Restore snapshot into source-device PVC"
          yq -i '.metadata.name = "source-device"' ~/csi-driver-host-path/examples/csi-block-pvc-restore.yaml
          yq -i '.spec.dataSource.name = "snap-1"' ~/csi-driver-host-path/examples/csi-block-pvc-restore.yaml
          yq -i '.spec.resources.requests.storage = "1Mi"' ~/csi-driver-host-path/examples/csi-block-pvc-restore.yaml
          kubectl create -f ~/csi-driver-host-path/examples/csi-block-pvc-restore.yaml
          
          echo "Create fresh target-device PVC"
          yq -i '.metadata.name = "target-device"' ~/csi-driver-host-path/examples/csi-pvc-block.yaml
          kubectl create -f ~/csi-driver-host-path/examples/csi-pvc-block.yaml
          
          echo "Create backup-app-client pod which mounts source-device and target-device PVCs"
          kubectl create -f deploy/example/backup-app/testdata/backup-app-pod.yaml
          kubectl wait  --for=condition=Ready pod/backup-app-client --timeout=300s

          echo "Copy snapshot-metadata-lister into backup-app-client pod"
          kubectl cp ./snapshot-metadata-lister default/backup-app-client:/snapshot-metadata-lister -c run-client
          kubectl cp ./snapshot-metadata-verifier default/backup-app-client:/snapshot-metadata-verifier -c run-client
          
          echo "Execute external-snapshot-metadata-client to list allocated blocks in snap-1"
          kubectl exec backup-app-client -- /snapshot-metadata-lister -max-results 10 -snapshot snap-1 -starting-offset 0 -namespace default -kubeconfig=""

          echo "Execute external-snapshot-metadata-client to list allocated blocks and verify the data allocated in snap-1"
          kubectl exec backup-app-client -- /snapshot-metadata-verifier -max-results 10 -snapshot snap-1 -starting-offset 0 -namespace default -kubeconfig="" -source-device-path /dev/source -target-device-path /dev/target

          echo "Write more data into source-device to test negative case"
          kubectl exec -i backup-app-client -- sh -c "dd if=/dev/urandom of=/dev/source bs=4K count=6 oflag=direct"
          
          echo "Execute external-snapshot-metadata-client to expect a failure"
          export failed=0
          kubectl exec backup-app-client -- /snapshot-metadata-verifier -max-results 10 -snapshot snap-1 -starting-offset 0 -namespace default -kubeconfig="" -source-device-path /dev/source -target-device-path /dev/target || { export failed=1; true;}
          if [ $failed -ne 1 ]; then
            echo "Command was expected to fail but it succeeded"
            exit 1
          else
            echo "Command failed as expected"
          fi

          echo "Delete backup-app-client pod and source-device and target-device PVCs"
          kubectl delete -f deploy/example/backup-app/testdata/backup-app-pod.yaml
          kubectl delete pvc source-device target-device
          
          echo "Write data into pod again"
          kubectl exec -i pod-raw -- sh -c "dd if=/dev/urandom of=/dev/block bs=4K count=5 oflag=direct"

          echo "Take snapshot snap-2"
          yq -i '.metadata.name = "snap-2"' ~/csi-driver-host-path/examples/csi-snapshot-v1.yaml
          yq -i '.spec.source.persistentVolumeClaimName = "pvc-raw"' ~/csi-driver-host-path/examples/csi-snapshot-v1.yaml
          kubectl create -f ~/csi-driver-host-path/examples/csi-snapshot-v1.yaml
          kubectl wait volumesnapshot snap-2 --for=jsonpath='{.status.readyToUse}'=true --timeout=300s

          echo "Restore snap-1 into target-device"
          yq -i '.metadata.name = "target-device"' ~/csi-driver-host-path/examples/csi-block-pvc-restore.yaml
          yq -i '.spec.dataSource.name = "snap-1"' ~/csi-driver-host-path/examples/csi-block-pvc-restore.yaml
          yq -i '.spec.resources.requests.storage = "1Mi"' ~/csi-driver-host-path/examples/csi-block-pvc-restore.yaml
          kubectl create -f ~/csi-driver-host-path/examples/csi-block-pvc-restore.yaml

          echo "Restore snap-2 into source-device"
          yq -i '.metadata.name = "source-device"' ~/csi-driver-host-path/examples/csi-block-pvc-restore.yaml
          yq -i '.spec.dataSource.name = "snap-2"' ~/csi-driver-host-path/examples/csi-block-pvc-restore.yaml
          yq -i '.spec.resources.requests.storage = "1Mi"' ~/csi-driver-host-path/examples/csi-block-pvc-restore.yaml
          kubectl create -f ~/csi-driver-host-path/examples/csi-block-pvc-restore.yaml
          
          echo "Create backup-app-client pod which mounts source-device and target-device PVCs"
          kubectl create -f deploy/example/backup-app/testdata/backup-app-pod.yaml
          kubectl wait  --for=condition=Ready pod/backup-app-client --timeout=300s

          echo "Copy snapshot-metadata-lister into backup-app-client pod"
          kubectl cp ./snapshot-metadata-lister default/backup-app-client:/snapshot-metadata-lister -c run-client
          kubectl cp ./snapshot-metadata-verifier default/backup-app-client:/snapshot-metadata-verifier -c run-client

          echo "Execute external-snapshot-metadata-client to list changed blocks between snap-1 and snap-2 using snapshot names"
          kubectl exec backup-app-client -- /snapshot-metadata-lister -max-results 10 -previous-snapshot snap-1 -snapshot snap-2 -starting-offset 0 -namespace default -kubeconfig=""

          VSC_NAME=`kubectl get volumesnapshot snap-1 -o jsonpath="{.status.boundVolumeSnapshotContentName}"`
          echo "VolumeSnapshotContent for VolumeSnapshot snap-1 is [$VSC_NAME]"
          SNAP_HANDLE=`kubectl get volumesnapshotcontent $VSC_NAME -o jsonpath="{.status.snapshotHandle}"`
          echo "CSI snapshot handle of VolumeSnapshot snap-1 is [$SNAP_HANDLE]"

          echo "Execute external-snapshot-metadata-client to list changed blocks between snap-1 and snap-2 using the CSI handle"
          kubectl exec backup-app-client -- /snapshot-metadata-lister -max-results 10 -previous-snapshot-id "$SNAP_HANDLE" -snapshot snap-2 -starting-offset 0 -namespace default -kubeconfig=""

          echo "Execute external-snapshot-metadata-client to verify the data between snap-1 and snap-2 using snapshot names"
          kubectl exec backup-app-client -- /snapshot-metadata-verifier -max-results 10 -previous-snapshot snap-1 -snapshot snap-2 -starting-offset 0 -namespace default -kubeconfig="" -source-device-path /dev/source -target-device-path /dev/target

          echo "Execute external-snapshot-metadata-client to verify the data between snap-1 and snap-2 using the CSI handle"
          kubectl exec backup-app-client -- /snapshot-metadata-verifier -max-results 10 -previous-snapshot-id "$SNAP_HANDLE" -snapshot snap-2 -starting-offset 0 -namespace default -kubeconfig="" -source-device-path /dev/source -target-device-path /dev/target

          echo "Write more data into source-device to test negative case"
          kubectl exec -i backup-app-client -- sh -c "dd if=/dev/urandom of=/dev/source bs=4K count=6 oflag=direct"
          
          echo "Execute external-snapshot-metadata-client to expect a failure"
          export failed=0
          kubectl exec backup-app-client -- /snapshot-metadata-verifier -max-results 10 -previous-snapshot snap-1 -snapshot snap-2 -starting-offset 0 -namespace default -kubeconfig="" -source-device-path /dev/source -target-device-path /dev/target || { export failed=1; true;}
          if [ $failed -ne 1 ]; then
            echo "Command was expected to fail but it succeeded"
            exit 1
          else
            echo "Command failed as expected"
          fi

      - name: Log the status of the failed driver pod
        if: ${{ failure() }}
        run: |
          kubectl get all -A

          kubectl get po -A --show-labels

          kubectl describe pod -n kube-system -l app.kubernetes.io/name=csi-hostpath-socat

          kubectl logs -n kube-system  -l app.kubernetes.io/name=csi-hostpath-socat

          kubectl describe pod backup-app-client

          kubectl logs backup-app-client

      - name: Setup tmate session to debug
        if: ${{ failure() }}
        uses: mxschmitt/action-tmate@v3